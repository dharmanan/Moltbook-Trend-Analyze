"""Trend analyzer â€” extracts keywords, topics, and patterns from Moltbook data."""

import json
import os
import re
from collections import Counter
from datetime import datetime
from typing import Any

from utils import log, save_analysis, load_latest, load_previous

# Load config
_settings_path = os.path.join(os.path.dirname(__file__), "..", "..", "config", "settings.json")
with open(_settings_path, "r") as f:
    _cfg = json.load(f)["analysis"]

STOP_WORDS = set(_cfg["stop_words"])
MIN_KW_LEN = _cfg["min_keyword_length"]
TOP_KW_COUNT = _cfg["top_keywords_count"]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Text Processing
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _extract_text(post: dict) -> str:
    """Extract all text content from a post."""
    parts = []
    for key in ("title", "content", "body", "text"):
        val = post.get(key, "")
        if val:
            parts.append(str(val))
    return " ".join(parts)


def _tokenize(text: str) -> list[str]:
    """Tokenize text into lowercase words, filtering noise."""
    text = re.sub(r"https?://\S+", "", text)       # remove URLs
    text = re.sub(r"```[\s\S]*?```", "", text)      # remove code blocks
    text = re.sub(r"`[^`]+`", "", text)              # remove inline code
    text = re.sub(r"[^a-zA-Z0-9\s\-]", " ", text)   # keep alphanumeric
    words = text.lower().split()
    return [
        w for w in words
        if len(w) >= MIN_KW_LEN and w not in STOP_WORDS and not w.isdigit()
    ]


def _extract_bigrams(words: list[str]) -> list[str]:
    """Extract meaningful two-word phrases."""
    bigrams = []
    for i in range(len(words) - 1):
        bg = f"{words[i]} {words[i + 1]}"
        if words[i] not in STOP_WORDS and words[i + 1] not in STOP_WORDS:
            bigrams.append(bg)
    return bigrams


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Keyword Analysis
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_keywords(posts: list[dict], top_n: int = TOP_KW_COUNT) -> list[dict]:
    """
    Extract top keywords from a list of posts.

    Returns: [{"keyword": str, "count": int, "frequency": float}, ...]
    """
    all_words = []
    for post in posts:
        text = _extract_text(post)
        all_words.extend(_tokenize(text))

    counter = Counter(all_words)
    total = len(all_words) or 1

    return [
        {
            "keyword": word,
            "count": count,
            "frequency": round(count / total, 4),
        }
        for word, count in counter.most_common(top_n)
    ]


def extract_bigram_topics(posts: list[dict], top_n: int = 15) -> list[dict]:
    """Extract top bigram (two-word) topics."""
    all_bigrams = []
    for post in posts:
        text = _extract_text(post)
        words = _tokenize(text)
        all_bigrams.extend(_extract_bigrams(words))

    counter = Counter(all_bigrams)
    total = len(all_bigrams) or 1

    return [
        {
            "topic": bg,
            "count": count,
            "frequency": round(count / total, 4),
        }
        for bg, count in counter.most_common(top_n)
    ]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Submolt Analysis
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def analyze_submolt_activity(data: dict) -> list[dict]:
    """
    Analyze activity levels across submolts.

    Returns sorted list of submolts by activity.
    """
    results = []
    feeds = data.get("submolt_feeds", {})

    for name, posts in feeds.items():
        post_count = len(posts) if isinstance(posts, list) else 0
        total_upvotes = sum(
            (p.get("upvotes", 0) or p.get("score", 0) or 0)
            for p in (posts if isinstance(posts, list) else [])
        )
        total_comments = sum(
            (p.get("comment_count", 0) or p.get("comments", 0) or 0)
            for p in (posts if isinstance(posts, list) else [])
        )

        results.append({
            "submolt": name,
            "post_count": post_count,
            "total_upvotes": total_upvotes,
            "total_comments": total_comments,
            "engagement_score": total_upvotes + total_comments * 2,
        })

    return sorted(results, key=lambda x: x["engagement_score"], reverse=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Agent Behavior Patterns
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def analyze_agent_patterns(posts: list[dict]) -> dict:
    """
    Analyze agent posting behaviors and patterns.

    Returns metrics about how agents behave on the platform.
    """
    agents = {}
    for post in posts:
        author = post.get("author", {})
        agent_name = author.get("name") if isinstance(author, dict) else str(author)
        if not agent_name:
            agent_name = post.get("author_name", "unknown")

        if agent_name not in agents:
            agents[agent_name] = {"posts": 0, "upvotes": 0}

        agents[agent_name]["posts"] += 1
        agents[agent_name]["upvotes"] += (
            post.get("upvotes", 0) or post.get("score", 0) or 0
        )

    # Compute stats
    post_counts = [a["posts"] for a in agents.values()]
    total_agents = len(agents)

    return {
        "unique_agents": total_agents,
        "total_posts_analyzed": len(posts),
        "avg_posts_per_agent": round(sum(post_counts) / max(total_agents, 1), 2),
        "top_posters": sorted(
            [{"name": k, **v} for k, v in agents.items()],
            key=lambda x: x["posts"],
            reverse=True,
        )[:10],
        "prolific_agents": len([c for c in post_counts if c >= 3]),
        "one_time_posters": len([c for c in post_counts if c == 1]),
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Trend Comparison
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def compare_trends(current_keywords: list[dict], previous_keywords: list[dict]) -> list[dict]:
    """
    Compare current keywords with previous period.

    Returns trend changes: rising, falling, new, stable.
    """
    prev_map = {k["keyword"]: k["count"] for k in previous_keywords}
    curr_map = {k["keyword"]: k["count"] for k in current_keywords}

    changes = []
    for kw in current_keywords:
        word = kw["keyword"]
        curr_count = kw["count"]
        prev_count = prev_map.get(word, 0)

        if prev_count == 0:
            change_type = "ðŸ†• new"
            change_pct = 100.0
        else:
            change_pct = round(((curr_count - prev_count) / prev_count) * 100, 1)
            if change_pct > 20:
                change_type = "ðŸ“ˆ rising"
            elif change_pct < -20:
                change_type = "ðŸ“‰ falling"
            else:
                change_type = "âž¡ï¸ stable"

        changes.append({
            "keyword": word,
            "current_count": curr_count,
            "previous_count": prev_count,
            "change_pct": change_pct,
            "trend": change_type,
        })

    return changes


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Full Analysis Pipeline
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_full_analysis(data: dict) -> dict:
    """
    Run the complete analysis pipeline on scraped data.

    Args:
        data: Output from full_scrape()

    Returns:
        Comprehensive analysis results dict
    """
    log.info("ðŸ”¬ Starting full analysis...")

    # Combine all posts for global analysis
    all_posts = []
    for key in ("hot_posts", "new_posts", "top_posts"):
        posts = data.get(key, [])
        if isinstance(posts, list):
            all_posts.extend(posts)

    # Deduplicate by post ID
    seen_ids = set()
    unique_posts = []
    for p in all_posts:
        pid = p.get("id") or p.get("_id") or id(p)
        if pid not in seen_ids:
            seen_ids.add(pid)
            unique_posts.append(p)

    log.info(f"  â†’ Analyzing {len(unique_posts)} unique posts...")

    # Run analyses
    keywords = extract_keywords(unique_posts)
    log.info(f"  â†’ Extracted {len(keywords)} top keywords")

    bigrams = extract_bigram_topics(unique_posts)
    log.info(f"  â†’ Extracted {len(bigrams)} bigram topics")

    submolt_activity = analyze_submolt_activity(data)
    log.info(f"  â†’ Analyzed {len(submolt_activity)} submolts")

    agent_patterns = analyze_agent_patterns(unique_posts)
    log.info(f"  â†’ Found {agent_patterns['unique_agents']} unique agents")

    # Try comparing with previous scrape
    trend_changes = []
    previous = load_previous("analyzed", "analysis")
    if previous and "keywords" in previous:
        trend_changes = compare_trends(keywords, previous["keywords"])
        log.info(f"  â†’ Compared with previous: {len(trend_changes)} trends tracked")
    else:
        log.info("  â†’ No previous data for comparison (first run)")

    # Build result
    analysis = {
        "analyzed_at": datetime.now().isoformat(),
        "total_unique_posts": len(unique_posts),
        "keywords": keywords,
        "bigram_topics": bigrams,
        "submolt_activity": submolt_activity,
        "agent_patterns": agent_patterns,
        "trend_changes": trend_changes,
        "metadata": data.get("metadata", {}),
    }

    # Save
    filepath = save_analysis(analysis, "analysis")
    log.info(f"âœ… Analysis complete! Saved to {filepath}")

    return analysis
